# Configuration for the fine-tuning script (fine_tune.py)

model_settings:
  # The name of the base model from Hugging Face to fine-tune.
  # Unsloth provides optimized models with `-bnb-4bit` for efficiency.
  name: "unsloth/Llama-3.2-1B-Instruct-unsloth-bnb-4bit"
  # name: "unsloth/llama-3-8b-Instruct-bnb-4bit"
  # Maximum sequence length. Unsloth supports RoPE scaling for longer contexts.
  max_seq_length: 2048
  # Whether to load the model in 4-bit precision for memory savings.
  load_in_4bit: true

lora_settings:
  # LoRA rank. Higher values mean more trainable parameters.
  r: 16
  # LoRA alpha. Acts as a scalar for the LoRA weights.
  lora_alpha: 32
  # Modules to target with LoRA adapters. Unsloth's defaults are usually optimal.
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  # Dropout probability for LoRA layers.
  lora_dropout: 0.05
  # Bias type for LoRA. "none" is recommended.
  bias: "none"
  # Important for memory saving.
  use_gradient_checkpointing: true

training_settings:
  # Directory to save the final LoRA adapters and training outputs.
  output_directory: "./outputs"
  # Number of training steps to perform.
  max_steps: 60
  # Batch size per GPU.
  per_device_train_batch_size: 2
  # Number of steps to accumulate gradients before updating weights.
  # Effective batch size = batch_size * gradient_accumulation_steps
  gradient_accumulation_steps: 4
  # Number of warmup steps for the learning rate scheduler.
  warmup_steps: 5
  # The learning rate for the optimizer.
  learning_rate: 0.0002 # 2e-4
  # Optimizer to use. Unsloth supports 8-bit optimizers like adamw_8bit.
  optim: "adamw_8bit"
  # Number of steps between logs.
  logging_steps: 1
  # Random seed for reproducibility.
  seed: 42

data_settings:
  # The name of the JSONL file generated by create_dataset.py.
  dataset_file: "dataset.jsonl"
