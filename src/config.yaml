# Markdown to Q&A Dataset Converter Configuration

# Input/Output Settings
input:
  # Path to markdown files (file or directory)
  path: "../data/raw/"
  # File extensions to process
  extensions: [".md", ".markdown"]
  # Maximum number of files to process (null for all)
  max_files: null
  # Recursive directory search
  recursive: true

output:
  # Output file path
  path: "../data/processed/qa_dataset.jsonl"
  # Output file format: json, jsonl, csv
  file_format: "jsonl"
  # Create backup if file exists
  backup_existing: true

# Model Configuration
model:
  # Ollama model name
  name: "llama3.2:latest"
  # API base URL for Ollama
  base_url: "http://eos.local:11434"
  # Request timeout in seconds
  timeout: 300
  # Temperature for generation (0.0 to 1.0)
  temperature: 0.7
  # Maximum tokens per request
  max_tokens: 2048

# Content Processing Settings
processing:
  # Minimum content chunk size (characters)
  min_chunk_size: 50
  # Maximum content chunk size (characters)
  max_chunk_size: 4000
  # Content types to extract
  extract_types:
    - headers_with_content
    - code_blocks
    - lists
    - tables
    - blockquotes
  # Code block minimum size
  min_code_block_size: 50
  # Skip empty content chunks
  skip_empty: true

# Q&A Generation Settings
qa_generation:
  # Dataset format: alpaca, chatml, sharegpt
  format: "alpaca"
  # Number of Q&A pairs per content chunk
  pairs_per_chunk:
    min: 1
    max: 5
  # Question types to generate
  question_types:
    - factual          # What, Who, When, Where
    - conceptual       # Why, How concepts work
    - procedural       # How to do something
    - comparative      # Compare/contrast
    - application      # Examples, use cases
    - analytical       # Analysis, evaluation
  # Difficulty levels
  difficulty_levels: ["easy", "medium", "hard"]
  # Quality filters
  quality_filters:
    min_question_length: 10
    min_answer_length: 20
    max_question_length: 200
    max_answer_length: 1000

# System Prompts
prompts:
  system_prompt: |
    You are an expert at creating high-quality question-answer pairs for LLM fine-tuning.
    
    Your task is to generate diverse, educational Q&A pairs from the given content.
    
    Guidelines:
    1. Create questions that test understanding, application, and analysis
    2. Vary question difficulty (easy, medium, hard)
    3. Include different question types: factual, conceptual, procedural
    4. Ensure answers are comprehensive but concise
    5. Use the exact content provided as context
    6. Generate diverse questions that would help someone learn the topic
    
    Question types to include:
    - What/Who/When/Where questions (factual)
    - How/Why questions (procedural/conceptual)
    - Compare/contrast questions
    - Application/example questions
    - Analysis and evaluation questions

  generation_prompt_template: |
    Generate high-quality question-answer pairs from this content:
    
    Content: {content}
    Category: {category}
    
    Requirements:
    - Create {min_pairs}-{max_pairs} diverse Q&A pairs
    - Include difficulty levels: {difficulty_levels}
    - Question types: {question_types}
    - Ensure questions are specific and answers are informative
    - Questions should be {min_q_len}-{max_q_len} characters
    - Answers should be {min_a_len}-{max_a_len} characters

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/converter.log"
  console: true

# Performance Settings
performance:
  # Number of concurrent processing tasks
  max_concurrent_tasks: 3
  # Delay between API calls (seconds)
  api_delay: 0.1
  # Retry settings for failed requests
  retry:
    max_attempts: 3
    delay: 1.0
    exponential_backoff: true
  # Memory management
  memory:
    # Process files in batches
    batch_size: 10
    # Clear cache after each batch
    clear_cache: true

# Validation Settings
validation:
  # Validate generated Q&A pairs
  enabled: true
  # Minimum quality score (0.0 to 1.0)
  min_quality_score: 0.6
  # Check for duplicate questions
  check_duplicates: true
  # Similarity threshold for duplicates
  duplicate_threshold: 0.8

# Export Settings
export:
  # Include metadata in output
  include_metadata: true
  # Metadata to include
  metadata_fields:
    - source_file
    - category
    - difficulty
    - question_type
    - created_at
    - model_used
  # Pretty print JSON output
  pretty_print: true
  # Include statistics file
  generate_stats: true
  stats_file: "./datasets/qa_dataset_stats.json"

# Unsloth Integration Settings
unsloth:
  # Model configuration for fine-tuning
  model_name: "unsloth/llama-3.2-3b-bnb-4bit"
  # LoRA configuration
  lora:
    r: 16
    lora_alpha: 16
    lora_dropout: 0.0
    target_modules: 
      - "q_proj"
      - "k_proj" 
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
  # Training configuration
  training:
    max_seq_length: 2048
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 4
    warmup_steps: 5
    max_steps: 100
    learning_rate: 2e-4
    weight_decay: 0.01
    lr_scheduler_type: "linear"
    output_dir: "./fine_tuned_model"
    eval_steps: 25
    save_steps: 50
    save_total_limit: 2