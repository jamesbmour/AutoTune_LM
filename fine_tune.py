"""
This script fine-tunes a Llama 3 model using Unsloth on the Q&A
dataset generated by `create_dataset.py`.

All settings for the model, LoRA adapters, and training process are
managed through the 'config_finetune.yaml' file.

Usage:
1. Ensure you have a generated 'dataset.jsonl' file.

2. Configure your settings in the 'config_finetune.yaml' file.
   Choose your model, LoRA parameters, and training settings.

3. Run the script:
   `python fine_tune.py`
"""
import os
import yaml
from unsloth import FastLanguageModel
import torch
from datasets import load_dataset
from transformers import TrainingArguments
from trl import SFTTrainer


# --- Helper Functions ---

def load_config(config_path: str = 'config_finetune.yaml') -> dict:
    """Loads the YAML configuration file for fine-tuning."""
    try:
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"Error: Configuration file '{config_path}' not found.")
        print("Please create a 'config_finetune.yaml' file.")
        exit(1)
    except yaml.YAMLError as e:
        print(f"Error parsing YAML file: {e}")
        exit(1)

def main():
    """Main function to run the fine-tuning process."""
    config = load_config()

    model_config = config.get('model_settings', {})
    lora_config = config.get('lora_settings', {})
    training_config = config.get('training_settings', {})
    data_config = config.get('data_settings', {})

    # 1. Load Model and Tokenizer
    print("Loading model and tokenizer with Unsloth...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_config.get('name', 'unsloth/llama-3-8b-Instruct-bnb-4bit'),
        max_seq_length=model_config.get('max_seq_length', 2048),
        dtype=None,  # Autodetect
        load_in_4bit=model_config.get('load_in_4bit', True),
    )
    print("Model and tokenizer loaded successfully.")

    # 2. Add LoRA Adapters
    print("Adding LoRA adapters to the model...")
    model = FastLanguageModel.get_peft_model(
        model,
        r=lora_config.get('r', 16),
        lora_alpha=lora_config.get('lora_alpha', 32),
        lora_dropout=lora_config.get('lora_dropout', 0.05),
        bias=lora_config.get('bias', 'none'),
        use_gradient_checkpointing=lora_config.get('use_gradient_checkpointing', True),
        random_state=training_config.get('seed', 42),
        target_modules=lora_config.get('target_modules', ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]),
        max_seq_length=model_config.get('max_seq_length', 2048),
    )
    print("LoRA adapters added.")

    # 3. Load and Format Dataset
    dataset_file = data_config.get('dataset_file', 'dataset.jsonl')
    if not os.path.exists(dataset_file):
        print(f"Error: Dataset file '{dataset_file}' not found.")
        print("Please run 'create_dataset.py' first to generate the dataset.")
        return

    print(f"Loading dataset from '{dataset_file}'...")
    dataset = load_dataset("json", data_files=dataset_file, split="train")

    # Alpaca prompt format
    alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

    EOS_TOKEN = tokenizer.eos_token
    def formatting_prompts_func(examples):
        instructions = examples["instruction"]
        inputs       = examples["input"]
        outputs      = examples["output"]
        texts = []
        for instruction, input, output in zip(instructions, inputs, outputs):
            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
            texts.append(text)
        return { "text" : texts, }
    
    dataset = dataset.map(formatting_prompts_func, batched = True,)
    print("Dataset loaded and formatted.")

    # 4. Setup Trainer
    print("Setting up SFTTrainer...")
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=model_config.get('max_seq_length', 2048),
        dataset_num_proc=2,
        packing=False, # Can make training faster
        args=TrainingArguments(
            per_device_train_batch_size=training_config.get('per_device_train_batch_size', 2),
            gradient_accumulation_steps=training_config.get('gradient_accumulation_steps', 4),
            warmup_steps=training_config.get('warmup_steps', 5),
            max_steps=training_config.get('max_steps', 60),
            learning_rate=training_config.get('learning_rate', 2e-4),
            fp16=not torch.cuda.is_bf16_supported(),
            bf16=torch.cuda.is_bf16_supported(),
            logging_steps=training_config.get('logging_steps', 1),
            optim=training_config.get('optim', 'adamw_8bit'),
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=training_config.get('seed', 42),
            output_dir=training_config.get('output_directory', 'outputs'),
        ),
    )
    print("Trainer setup complete.")

    # 5. Start Training
    print("\nStarting model training...")
    trainer_stats = trainer.train()
    print("Training complete.")

    # 6. Save Final Model
    output_dir = training_config.get('output_directory', 'outputs')
    print(f"Saving fine-tuned model to '{output_dir}'...")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print("Model saved successfully.")
    
    # 7. Simple Inference Test
    print("\nRunning a simple inference test...")
    prompt = alpaca_prompt.format(
        "What is Unsloth?", # instruction
        "", # input
        "", # output - leave this blank for the model to generate
    )

    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)
    response = tokenizer.batch_decode(outputs)
    
    print("\n--- INFERENCE TEST ---")
    print("PROMPT:")
    print(prompt.replace(tokenizer.eos_token, ''))
    print("\nMODEL RESPONSE:")
    print(response[0].split("### Response:")[1].strip().replace(tokenizer.eos_token, ''))
    print("------------------------")

if __name__ == "__main__":
    main()
